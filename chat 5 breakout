#!/usr/bin/env python3
# breakout_detection.py – automatische CUT-Suche

from __future__ import annotations
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple
import re
import pandas as pd

# ============================ Konfiguration ============================

EXCEL_FILES = ["SMA 1_10.xlsx", "SMA 11_20.xlsx", "SMA 21_28.xlsx"]
DEFAULT_LOOKBACK = 4
DEFAULT_OUT_CSV  = "fibbreakout-fiborigin.csv"

COLUMN_ALIASES: Dict[str, List[str]] = {
    "time":  ["time", "Time", "datetime", "timestamp", "date", "Date"],
    "open":  ["open", "Open"],
    "high":  ["high", "High"],
    "low":   ["low", "Low"],
    "close": ["close", "Close", "c"],
    "upper": ["Upper", "upper", "keltner_upper", "BB_Upper", "UpperBand"],
    "lower": ["Lower", "lower", "keltner_lower", "BB_Lower", "LowerBand"],
    "smma75": ["SMMA.1"],  # 75er SMA MUSS vorhanden sein
}

CUT_ALIASES: Dict[str, List[str]] = {
    "pair":      ["Pair", "pair", "symbol", "sheet"],
    "start":     ["Start Date", "start", "from", "Start"],
    "end":       ["End Date", "end", "to", "End"],
    "direction": ["Direction", "direction", "signal"],
}

# ============================ Hilfsfunktionen ============================

def _norm(s: str) -> str:
    return re.sub(r"[^a-z0-9]+", "", str(s).strip().lower())

def _build_alias_map(aliases: Dict[str, List[str]]) -> Dict[str, str]:
    m = {}
    for canon, alist in aliases.items():
        for a in alist:
            m[_norm(a)] = canon
    return m

def _normalize_columns(df: pd.DataFrame, aliases: Dict[str, List[str]]) -> pd.DataFrame:
    amap = _build_alias_map(aliases)
    df.columns = [re.sub(r"\s+", " ", str(c)).strip() for c in df.columns]
    return df.rename(columns={c: amap.get(_norm(c), c) for c in df.columns})

def _ensure_utc(ts) -> pd.Timestamp:
    t = pd.to_datetime(ts, errors="coerce")
    if pd.isna(t):
        return t
    if getattr(t, "tz", None) is None:
        return t.tz_localize("UTC")
    return t.tz_convert("UTC")

def _end_exclusive_if_date_like(t: pd.Timestamp) -> pd.Timestamp:
    if pd.isna(t):
        return t
    if t.hour == 0 and t.minute == 0 and t.second == 0 and t.nanosecond == 0:
        return t + pd.Timedelta(days=1)
    return t

# ============================ Daten laden ============================

class PriceBook:
    def __init__(self, excel_files: Iterable[str] = EXCEL_FILES):
        self.files = [Path(f) for f in excel_files]
        self.sheet_index = self._index()

    def _index(self) -> Dict[str, Path]:
        idx = {}
        for f in self.files:
            if not f.exists():
                continue
            xls = pd.ExcelFile(f)
            for sheet in xls.sheet_names:
                idx[sheet] = f
        if not idx:
            raise FileNotFoundError("Keine Excel-Dateien gefunden.")
        return idx

    def load(self, pair: str) -> pd.DataFrame:
        key = next((s for s in self.sheet_index if s.lower() == pair.lower()), None)
        if key is None:
            raise ValueError(f"Unbekanntes Paar: {pair}")
        fpath = self.sheet_index[key]
        df = pd.read_excel(fpath, sheet_name=key)
        df = _normalize_columns(df, COLUMN_ALIASES)

        required = {"time", "high", "low", "close", "upper", "lower", "smma75"}
        missing = [c for c in required if c not in df.columns]
        if missing:
            raise ValueError(f"[{pair}] Fehlende Spalten {missing}. SMMA.1 muss vorhanden sein.")

        for col in ["high", "low", "close", "upper", "lower", "smma75"]:
            df[col] = pd.to_numeric(df[col], errors="coerce")
        df["time"] = pd.to_datetime(df["time"], utc=True, errors="coerce")
        df = df.dropna(subset=["time"]).sort_values("time").reset_index(drop=True)
        return df

# ============================ Detection-Logik ============================

def _runs(mask: pd.Series) -> pd.Series:
    return (mask.fillna(False) != mask.fillna(False).shift()).cumsum()

def _find_blocks(mask: pd.Series):
    out = []
    groups = _runs(mask)
    for gid in groups[mask].unique():
        idx = mask.index[mask & (groups == gid)]
        out.append((idx[0], idx[-1]))
    return out

def _extreme(df: pd.DataFrame, si: int, ei: int, lookback: int, kind: str):
    lo = max(0, si - lookback)
    hi = min(len(df) - 1, ei + lookback)
    win = df.loc[lo:hi]
    if kind == "high":
        idx = win["high"].idxmax()
        return df.loc[idx, "time"], float(df.loc[idx, "high"])
    else:
        idx = win["low"].idxmin()
        return df.loc[idx, "time"], float(df.loc[idx, "low"])

def _signals_for_mode(df: pd.DataFrame, mode: str, direction: str, windows, lookback: int):
    c, u, l, m = df["close"], df["upper"], df["lower"], df["smma75"]

    if mode == "breakout":
        stay_mask = (c > u) if direction == "long" else (c < l)
        start_mask = ((c > u) & (c > m)) if direction == "long" else ((c < l) & (c < m))
        name = "Long Breakout" if direction == "long" else "Short Breakout"
        extreme_kind = "high" if direction == "long" else "low"
    elif mode == "origin":
        stay_mask = (c < l) if direction == "long" else (c > u)
        start_mask = stay_mask.copy()
        name = "Fib Long Origin" if direction == "long" else "Fib Short Origin"
        extreme_kind = "low" if direction == "long" else "high"
    else:
        raise ValueError("Mode muss 'breakout' oder 'origin' sein.")

    out = []
    for blk_start, blk_end in _find_blocks(stay_mask):
        if mode == "breakout":
            cand = start_mask.loc[blk_start:blk_end]
            if not cand.any():
                continue
            start_i = int(cand[cand].index[0])
        else:
            start_i = int(blk_start)

        end_i = int(blk_end)
        st, et = df.loc[start_i, "time"], df.loc[end_i, "time"]

        if windows and not any((st >= ws) and (st < we) for (ws, we) in windows):
            continue

        xt_t, xt_p = _extreme(df, start_i, end_i, lookback, extreme_kind)

        out.append({
            "signal_type": name,
            "direction": "Long" if direction == "long" else "Short",
            "start_time": st,
            "end_time": et,
            "extreme_time": xt_t,
            "extreme_price": xt_p,
        })
    return out

# ============================ CUT Loader ============================

def _find_cut_path(cli_path: Optional[str]) -> Path:
    if cli_path:
        p = Path(cli_path)
        if p.exists():
            return p
    search_names = [
        "COTsignalsoutput.tsv", "COTsignalsoutput.csv",
        "COTsignalsoutputs.tsv", "COTsignalsoutputs.csv",
        "CUTsignalsoutput.tsv", "CUTsignalsoutput.csv",
        "CutSignalsOutput.tsv", "CutSignalsOutputs.tsv",
    ]
    for name in search_names:
        p = Path(name)
        if p.exists():
            return p
    raise FileNotFoundError("Keine CUT-Datei gefunden.")

def load_cut(path: Optional[str]) -> pd.DataFrame:
    p = _find_cut_path(path)
    df = pd.read_csv(p, sep=None, engine="python")
    df = _normalize_columns(df, CUT_ALIASES)
    required = {"pair", "start", "end", "direction"}
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise ValueError(f"CUT-Datei fehlt Spalten {missing}")
    df["start"] = df["start"].apply(_ensure_utc)
    df["end"] = df["end"].apply(_ensure_utc).apply(_end_exclusive_if_date_like)
    df["direction"] = (
        df["direction"].astype(str).str.strip().str.lower()
        .map({"long": "long", "short": "short", "buy": "long", "sell": "short"})
    )
    df["pair"] = df["pair"].astype(str)
    return df

# ============================ Output ============================

def _save_outputs(df: pd.DataFrame, csv_path: str):
    df.to_csv(csv_path, index=False, sep=";")
    print(f"✅ Output gespeichert: {csv_path}")

# ============================ Main ====================================

def main():
    import argparse
    p = argparse.ArgumentParser()
    p.add_argument("--cut", help="Pfad zur CUT-Datei (TSV/CSV). Wenn leer, wird automatisch gesucht.")
    p.add_argument("--lookback", type=int, default=DEFAULT_LOOKBACK)
    p.add_argument("--out", default=DEFAULT_OUT_CSV)
    args = p.parse_args()

    book = PriceBook()
    cut_df = load_cut(args.cut)

    rows = []
    for _, r in cut_df.iterrows():
        df_prices = book.load(r["pair"])
        windows = [(r["start"], r["end"])]
        for mode in ("breakout", "origin"):
            rows += _signals_for_mode(df_prices, mode, r["direction"], windows, args.lookback)

    out_df = pd.DataFrame(rows)
    _save_outputs(out_df, args.out)

if __name__ == "__main__":
    main()
